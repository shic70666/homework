{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torchvision.datasets as normal_datasets\n",
    "# import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHM_Dataset(Dataset):\n",
    "    \"\"\" Prepare dataset for pytorch\n",
    "        Ref: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, case, data_file):\n",
    "        self.case = case\n",
    "        self.data_file = Path(data_file)\n",
    "        self.data_df = pd.read_json(self.data_file)\n",
    "        self.data = self.data_df.iloc[:, 1] # @Chen here I ONLY take a column of data as an example\n",
    "        self.labels = pd.DataFrame([1,]*self.data_df.shape[0]) # ONLY take a column as an example\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = np.array(self.labels.iloc[index])\n",
    "        feature = np.array(self.data.iloc[index])\n",
    "        return feature, label\n",
    "\n",
    "shmDS = SHM_Dataset(1, \"~/Codes/homework/SHM/shm01s.json\")\n",
    "\n",
    "train_loader = DataLoader(shmDS, batch_size=32, shuffle=True) # split samples into mini-batches and reshuffle the data to reduce overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look on a single sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 8, 16])\n",
      "Labels batch shape: torch.Size([32, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "fig, axis = plt.subplots()\n",
    "axis.imshow(img, cmap=\"gray\")\n",
    "axis.set(title=f\"Label: {label}\", xticks=range(16), yticks=range(8))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the neural network\n",
    "\n",
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-layer convolution\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Quick build with sequence tools\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(7 * 7 * 32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = out.view(out.size(0), -1)  # reshape\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "cnn = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "num_epochs = 1\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> tensor([[[0.0026, 0.0027, 0.0024,  ..., 0.0023, 0.0024, 0.0023],\n",
      "         [0.0023, 0.0023, 0.0024,  ..., 0.0024, 0.0024, 0.0024],\n",
      "         [0.0023, 0.0024, 0.0024,  ..., 0.0024, 0.0023, 0.0023],\n",
      "         ...,\n",
      "         [0.0023, 0.0024, 0.0023,  ..., 0.0023, 0.0023, 0.0023],\n",
      "         [0.0023, 0.0023, 0.0023,  ..., 0.0023, 0.0023, 0.0024],\n",
      "         [0.0024, 0.0023, 0.0024,  ..., 0.0024, 0.0024, 0.0023]],\n",
      "\n",
      "        [[0.0024, 0.0023, 0.0023,  ..., 0.0023, 0.0024, 0.0024],\n",
      "         [0.0024, 0.0024, 0.0023,  ..., 0.0023, 0.0023, 0.0024],\n",
      "         [0.0023, 0.0023, 0.0024,  ..., 0.0024, 0.0024, 0.0024],\n",
      "         ...,\n",
      "         [0.0023, 0.0023, 0.0023,  ..., 0.0024, 0.0023, 0.0024],\n",
      "         [0.0024, 0.0024, 0.0023,  ..., 0.0023, 0.0023, 0.0023],\n",
      "         [0.0024, 0.0024, 0.0024,  ..., 0.0023, 0.0024, 0.0024]],\n",
      "\n",
      "        [[0.0024, 0.0023, 0.0023,  ..., 0.0024, 0.0024, 0.0024],\n",
      "         [0.0023, 0.0022, 0.0023,  ..., 0.0024, 0.0023, 0.0023],\n",
      "         [0.0023, 0.0023, 0.0023,  ..., 0.0023, 0.0023, 0.0023],\n",
      "         ...,\n",
      "         [0.0023, 0.0023, 0.0024,  ..., 0.0023, 0.0023, 0.0023],\n",
      "         [0.0023, 0.0023, 0.0023,  ..., 0.0024, 0.0024, 0.0024],\n",
      "         [0.0024, 0.0023, 0.0023,  ..., 0.0023, 0.0023, 0.0024]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0025, 0.0024, 0.0023,  ..., 0.0024, 0.0023, 0.0023],\n",
      "         [0.0024, 0.0023, 0.0023,  ..., 0.0024, 0.0023, 0.0023],\n",
      "         [0.0023, 0.0023, 0.0023,  ..., 0.0023, 0.0023, 0.0023],\n",
      "         ...,\n",
      "         [0.0024, 0.0023, 0.0023,  ..., 0.0023, 0.0024, 0.0023],\n",
      "         [0.0023, 0.0024, 0.0024,  ..., 0.0024, 0.0023, 0.0023],\n",
      "         [0.0023, 0.0024, 0.0024,  ..., 0.0024, 0.0023, 0.0023]],\n",
      "\n",
      "        [[0.0023, 0.0024, 0.0024,  ..., 0.0024, 0.0024, 0.0023],\n",
      "         [0.0023, 0.0023, 0.0023,  ..., 0.0023, 0.0023, 0.0024],\n",
      "         [0.0024, 0.0023, 0.0023,  ..., 0.0023, 0.0023, 0.0023],\n",
      "         ...,\n",
      "         [0.0024, 0.0024, 0.0023,  ..., 0.0024, 0.0024, 0.0024],\n",
      "         [0.0023, 0.0023, 0.0023,  ..., 0.0023, 0.0023, 0.0024],\n",
      "         [0.0024, 0.0024, 0.0024,  ..., 0.0023, 0.0024, 0.0024]],\n",
      "\n",
      "        [[0.0024, 0.0024, 0.0023,  ..., 0.0023, 0.0024, 0.0024],\n",
      "         [0.0023, 0.0024, 0.0024,  ..., 0.0023, 0.0023, 0.0023],\n",
      "         [0.0024, 0.0024, 0.0024,  ..., 0.0024, 0.0023, 0.0023],\n",
      "         ...,\n",
      "         [0.0024, 0.0023, 0.0023,  ..., 0.0023, 0.0024, 0.0024],\n",
      "         [0.0024, 0.0024, 0.0024,  ..., 0.0024, 0.0024, 0.0023],\n",
      "         [0.0023, 0.0023, 0.0023,  ..., 0.0023, 0.0023, 0.0024]]],\n",
      "       dtype=torch.float64)\n",
      "<class 'torch.Tensor'> tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]])\n",
      "[ OK ] at this step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[ OK ] at this step\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Forward + Backward + Optimize \u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m cnn(images)\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(outputs, labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        print(type(images), images)\n",
    "        print(type(labels), labels)\n",
    "        print(\"[ OK ] at this step\") #TODO: @Chen apdat the CNN to fit your \"image\" size (8x16) as you said this afternoon\n",
    "\n",
    "        # Forward + Backward + Optimize \n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(images)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test Accuracy: 98 %\n"
     ]
    }
   ],
   "source": [
    "cnn.eval()  # Change to test form, application scenarios such as: dropout\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images)\n",
    "    labels = Variable(labels)\n",
    "\n",
    "    outputs = cnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels.data).sum()\n",
    "\n",
    "print(' Test Accuracy: %d %%' % (100 * correct / total))\n",
    "\n",
    "# Save the Trained Model\n",
    "torch.save(cnn.state_dict(), 'cnn.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

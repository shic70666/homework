{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb14c1f3830>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Hyper Parameters\n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHM_Dataset(Dataset):\n",
    "    \"\"\" Prepare dataset for pytorch\n",
    "        Ref: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, case, data_file,transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])): \n",
    "        self.case = case\n",
    "        self.data_file = Path(data_file)\n",
    "        self.data_df = pd.read_json(self.data_file, dtype=np.array)\n",
    "        # self.data = self.data_df.cat()\n",
    "        self.data = self.data_df.stack()\n",
    "        self.labels = pd.DataFrame([self.case,]*self.data_df.shape[0]*self.data_df.shape[1])\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = np.array(self.labels.iloc[index])\n",
    "        feature = np.array(self.data.iloc[index])\n",
    "        feature = self.transform(feature)\n",
    "        # print(f\"feature: {feature.shape}, label: {label.shape}\")\n",
    "        return feature, label\n",
    "\n",
    "\n",
    "# shmDS = SHM_Dataset(1, \"~/Codes/homework/data/SHM/shm01s.json\")\n",
    "# print(\"There is\", len(shmDS), \"samples in the given dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shmDS_1 = SHM_Dataset(1, \"~/Codes/homework/data/SHM/shm01s.json\")\n",
    "shmDS_2 = SHM_Dataset(2, \"~/Codes/homework/data/SHM/shm02s.json\")\n",
    "shmDS_3 = SHM_Dataset(3, \"~/Codes/homework/data/SHM/shm03s.json\")\n",
    "shmDS_4 = SHM_Dataset(4, \"~/Codes/homework/data/SHM/shm04s.json\")\n",
    "shmDS_5 = SHM_Dataset(5, \"~/Codes/homework/data/SHM/shm05s.json\")\n",
    "shmDS_6 = SHM_Dataset(6, \"~/Codes/homework/data/SHM/shm06s.json\")\n",
    "shmDS_7 = SHM_Dataset(7, \"~/Codes/homework/data/SHM/shm07s.json\")\n",
    "shmDS_8 = SHM_Dataset(8, \"~/Codes/homework/data/SHM/shm08s.json\")\n",
    "shmDS_9 = SHM_Dataset(9, \"~/Codes/homework/data/SHM/shm09s.json\")\n",
    "shmDS = shmDS_1 + shmDS_2 + shmDS_3 + shmDS_4 + shmDS_5 + shmDS_6 + shmDS_7 + shmDS_8 + shmDS_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 3744 samples in the given dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"There is\", len(shmDS_6), \"samples in the given dataset\")\n",
    "# print(shmDS_6.__getitem__(11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look on a single sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([8, 1, 16, 16])\n",
      "Labels batch shape: torch.Size([8, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader = DataLoader(shmDS, batch_size=batch_size, shuffle=True) # split samples into mini-batches and reshuffle the data to reduce overfitting\n",
    "test_loader = DataLoader(shmDS, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "# train_features = train_features.unsqueeze(dim=1)\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "fig, axis = plt.subplots()\n",
    "axis.imshow(img, cmap=\"gray\")\n",
    "\n",
    "axis.set(title=f\"Label: {label}\", xticks=range(16), yticks=range(8))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the neural network\n",
    "\n",
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-layer convolution\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Quick build with sequence tools\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(4, 8, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(32 * 1 * 1, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        # print(x.shape)\n",
    "        x = x.view(-1, 32 * 1 * 1) \n",
    "        # print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "cnn = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Iter [100/3084] Loss: 0.0355\n",
      "Epoch [1/10], Iter [200/3084] Loss: 0.0093\n",
      "Epoch [1/10], Iter [300/3084] Loss: 0.0048\n",
      "Epoch [1/10], Iter [400/3084] Loss: 0.0042\n",
      "Epoch [1/10], Iter [500/3084] Loss: 0.0021\n",
      "Epoch [1/10], Iter [600/3084] Loss: 0.0015\n",
      "Epoch [1/10], Iter [700/3084] Loss: 0.0012\n",
      "Epoch [1/10], Iter [800/3084] Loss: 0.0009\n",
      "Epoch [1/10], Iter [900/3084] Loss: 0.0009\n",
      "Epoch [1/10], Iter [1000/3084] Loss: 0.0008\n",
      "Epoch [1/10], Iter [1100/3084] Loss: 0.0006\n",
      "Epoch [1/10], Iter [1200/3084] Loss: 0.0006\n",
      "Epoch [1/10], Iter [1300/3084] Loss: 0.0004\n",
      "Epoch [1/10], Iter [1400/3084] Loss: 0.0004\n",
      "Epoch [1/10], Iter [1500/3084] Loss: 0.0003\n",
      "Epoch [1/10], Iter [1600/3084] Loss: 0.0004\n",
      "Epoch [1/10], Iter [1700/3084] Loss: 0.0003\n",
      "Epoch [1/10], Iter [1800/3084] Loss: 0.0003\n",
      "Epoch [1/10], Iter [1900/3084] Loss: 0.0002\n",
      "Epoch [1/10], Iter [2000/3084] Loss: 0.0002\n",
      "Epoch [1/10], Iter [2100/3084] Loss: 0.0001\n",
      "Epoch [1/10], Iter [2200/3084] Loss: 0.0001\n",
      "Epoch [1/10], Iter [2300/3084] Loss: 0.0001\n",
      "Epoch [1/10], Iter [2400/3084] Loss: 0.0001\n",
      "Epoch [1/10], Iter [2500/3084] Loss: 0.0001\n",
      "Epoch [1/10], Iter [2600/3084] Loss: 0.0001\n",
      "Epoch [1/10], Iter [2700/3084] Loss: 0.0001\n",
      "Epoch [1/10], Iter [2800/3084] Loss: 0.0001\n",
      "Epoch [1/10], Iter [2900/3084] Loss: 0.0001\n",
      "Epoch [1/10], Iter [3000/3084] Loss: 0.0002\n",
      "Epoch [2/10], Iter [100/3084] Loss: 0.0001\n",
      "Epoch [2/10], Iter [200/3084] Loss: 0.0001\n",
      "Epoch [2/10], Iter [300/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [400/3084] Loss: 0.0001\n",
      "Epoch [2/10], Iter [500/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [600/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [700/3084] Loss: 0.0001\n",
      "Epoch [2/10], Iter [800/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [900/3084] Loss: 0.0001\n",
      "Epoch [2/10], Iter [1000/3084] Loss: 0.0001\n",
      "Epoch [2/10], Iter [1100/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [1200/3084] Loss: 0.0001\n",
      "Epoch [2/10], Iter [1300/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [1400/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [1500/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [1600/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [1700/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [1800/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [1900/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [2000/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [2100/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [2200/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [2300/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [2400/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [2500/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [2600/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [2700/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [2800/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [2900/3084] Loss: 0.0000\n",
      "Epoch [2/10], Iter [3000/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [100/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [200/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [300/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [400/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [500/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [600/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [700/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [800/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [900/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [1000/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [1100/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [1200/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [1300/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [1400/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [1500/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [1600/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [1700/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [1800/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [1900/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [2000/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [2100/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [2200/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [2300/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [2400/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [2500/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [2600/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [2700/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [2800/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [2900/3084] Loss: 0.0000\n",
      "Epoch [3/10], Iter [3000/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [100/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [200/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [300/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [400/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [500/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [600/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [700/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [800/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [900/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [1000/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [1100/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [1200/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [1300/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [1400/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [1500/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [1600/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [1700/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [1800/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [1900/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [2000/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [2100/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [2200/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [2300/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [2400/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [2500/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [2600/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [2700/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [2800/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [2900/3084] Loss: 0.0000\n",
      "Epoch [4/10], Iter [3000/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [100/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [200/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [300/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [400/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [500/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [600/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [700/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [800/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [900/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [1000/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [1100/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [1200/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [1300/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [1400/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [1500/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [1600/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [1700/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [1800/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [1900/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [2000/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [2100/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [2200/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [2300/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [2400/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [2500/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [2600/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [2700/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [2800/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [2900/3084] Loss: 0.0000\n",
      "Epoch [5/10], Iter [3000/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [100/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [200/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [300/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [400/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [500/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [600/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [700/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [800/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [900/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [1000/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [1100/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [1200/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [1300/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [1400/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [1500/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [1600/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [1700/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [1800/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [1900/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [2000/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [2100/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [2200/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [2300/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [2400/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [2500/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [2600/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [2700/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [2800/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [2900/3084] Loss: 0.0000\n",
      "Epoch [6/10], Iter [3000/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [100/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [200/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [300/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [400/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [500/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [600/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [700/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [800/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [900/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [1000/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [1100/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [1200/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [1300/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [1400/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [1500/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [1600/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [1700/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [1800/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [1900/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [2000/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [2100/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [2200/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [2300/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [2400/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [2500/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [2600/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [2700/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [2800/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [2900/3084] Loss: 0.0000\n",
      "Epoch [7/10], Iter [3000/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [100/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [200/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [300/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [400/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [500/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [600/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [700/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [800/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [900/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [1000/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [1100/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [1200/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [1300/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [1400/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [1500/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [1600/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [1700/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [1800/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [1900/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [2000/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [2100/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [2200/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [2300/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [2400/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [2500/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [2600/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [2700/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [2800/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [2900/3084] Loss: 0.0000\n",
      "Epoch [8/10], Iter [3000/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [100/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [200/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [300/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [400/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [500/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [600/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [700/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [800/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [900/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [1000/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [1100/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [1200/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [1300/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [1400/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [1500/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [1600/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [1700/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [1800/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [1900/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [2000/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [2100/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [2200/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [2300/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [2400/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [2500/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [2600/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [2700/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [2800/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [2900/3084] Loss: 0.0000\n",
      "Epoch [9/10], Iter [3000/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [100/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [200/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [300/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [400/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [500/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [600/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [700/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [800/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [900/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [1000/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [1100/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [1200/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [1300/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [1400/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [1500/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [1600/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [1700/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [1800/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [1900/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [2000/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [2100/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [2200/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [2300/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [2400/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [2500/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [2600/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [2700/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [2800/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [2900/3084] Loss: 0.0000\n",
      "Epoch [10/10], Iter [3000/3084] Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (train_features, train_labels) in enumerate(train_loader):\n",
    "        train_features = Variable(train_features)\n",
    "        # images = images.unsqueeze(dim=1)\n",
    "        train_features = train_features.float()\n",
    "        train_labels = Variable(train_labels)\n",
    "        # print(type(images), images)\n",
    "        # print(type(labels), labels)\n",
    "        # print(\"[ OK ] at this step\") \n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(train_features)\n",
    "        # loss = loss_func(outputs, labels)\n",
    "        loss = loss_func(outputs, torch.max(train_labels, 1)[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, len(shmDS) // batch_size, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test Accuracy: 174 %\n"
     ]
    }
   ],
   "source": [
    "cnn.eval()  # Change to test form, application scenarios such as: dropout\n",
    "correct = 0\n",
    "total = 0\n",
    "for test_features, test_labels in test_loader:\n",
    "    test_features = Variable(test_features)\n",
    "    # images = images.unsqueeze(dim=1)\n",
    "    test_features = test_features.float()\n",
    "    test_labels = Variable(test_labels)\n",
    "    # print(x.shape)\n",
    "\n",
    "\n",
    "    outputs = cnn(test_features)\n",
    "    # print(outputs.type)\n",
    "    _, predicted = torch.max(outputs.data, 0)\n",
    "    # print(outputs.data)\n",
    "    # print(predicted)\n",
    "    predicted = predicted +1\n",
    "    total += test_labels.size(0)\n",
    "    # print('ok for this step', total)\n",
    "    # print(test_labels.data)\n",
    "    correct += (predicted == test_labels.data).sum()\n",
    "    \n",
    "    # print('okkkkkkkkkkkkkk for this step', correct)\n",
    "\n",
    "print(' Test Accuracy: %d %%' % (100 * correct / total))\n",
    "\n",
    "# Save the Trained Model\n",
    "# torch.save(cnn.state_dict(), 'cnn.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
